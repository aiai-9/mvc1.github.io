<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>StyleTTS2 Voice Samples</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <h1>Audio Samples from "StyleTTS2: Towards Human-Level Text-to-Speech"</h1>
        <p>
            <strong>Abstract:</strong> In this paper, we present StyleTTS2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation.
            <br><br>
            <a href="https://arxiv.org/abs/2306.07691" target="_blank">arXiv</a> | 
            <a href="https://github.com/yl4579/StyleTTS2" target="_blank">GitHub Repo</a>
        </p>
    </header>

     <section id="simple-samples"></section>

    <h2 class="section-title">1. Single-Speaker (LJSpeech, In-Distribution Texts)</h2> -->
<!--     <hr> -->
     <section id="text-samples"></section>

    <footer>
        <p>Hosted on GitHub Pages</p>
    </footer>

    <script src="script.js"></script>
</body>
</html>
